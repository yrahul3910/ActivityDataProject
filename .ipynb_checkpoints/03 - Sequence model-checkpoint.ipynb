{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model here is inspired from [ML Mastery's article on text generation](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sequences.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(data.keys())\n",
    "Y = list(data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [list(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 17, 16, 13, 7]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_unique = len(np.unique(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(X, (len(X), len(X[0]), 1))\n",
    "X = X / float(n_unique)\n",
    "y = np_utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"checkpoint-{epoch:02d}-{loss:.4f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min', period=20)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3193/3193 [==============================] - 9s 3ms/step - loss: 2.8718\n",
      "Epoch 2/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7772\n",
      "Epoch 3/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.7740\n",
      "Epoch 4/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 2.7697\n",
      "Epoch 5/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7678\n",
      "Epoch 6/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.7699\n",
      "Epoch 7/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7651\n",
      "Epoch 8/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7607\n",
      "Epoch 9/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.7552\n",
      "Epoch 10/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.7507\n",
      "Epoch 11/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.7484\n",
      "Epoch 12/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.7440\n",
      "Epoch 13/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.7408\n",
      "Epoch 14/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7316\n",
      "Epoch 15/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.7372\n",
      "Epoch 16/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7332\n",
      "Epoch 17/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.7303\n",
      "Epoch 18/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7221\n",
      "Epoch 19/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7258\n",
      "Epoch 20/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7214\n",
      "\n",
      "Epoch 00020: loss improved from inf to 2.72140, saving model to checkpoint-20-2.7214.h5\n",
      "Epoch 21/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.7245\n",
      "Epoch 22/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.7170\n",
      "Epoch 23/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.7181\n",
      "Epoch 24/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7069\n",
      "Epoch 25/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7131\n",
      "Epoch 26/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7094\n",
      "Epoch 27/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7083\n",
      "Epoch 28/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7037\n",
      "Epoch 29/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7036\n",
      "Epoch 30/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.7008\n",
      "Epoch 31/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.7032\n",
      "Epoch 32/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.7019\n",
      "Epoch 33/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 2.7021\n",
      "Epoch 34/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.6985\n",
      "Epoch 35/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.6953\n",
      "Epoch 36/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.6910\n",
      "Epoch 37/500\n",
      "3193/3193 [==============================] - 8s 2ms/step - loss: 2.6908\n",
      "Epoch 38/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.6896\n",
      "Epoch 39/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.6869\n",
      "Epoch 40/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 2.6849\n",
      "\n",
      "Epoch 00040: loss improved from 2.72140 to 2.68489, saving model to checkpoint-40-2.6849.h5\n",
      "Epoch 41/500\n",
      "3193/3193 [==============================] - 8s 3ms/step - loss: 2.6771\n",
      "Epoch 42/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 2.6780\n",
      "Epoch 43/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.6756\n",
      "Epoch 44/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.6681\n",
      "Epoch 45/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.6660\n",
      "Epoch 46/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.6545\n",
      "Epoch 47/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.6529\n",
      "Epoch 48/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.6453\n",
      "Epoch 49/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.6394\n",
      "Epoch 50/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.6239\n",
      "Epoch 51/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.6136\n",
      "Epoch 52/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.6191\n",
      "Epoch 53/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.6049\n",
      "Epoch 54/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.5970\n",
      "Epoch 55/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.5846\n",
      "Epoch 56/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.5720\n",
      "Epoch 57/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.5618\n",
      "Epoch 58/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.5615\n",
      "Epoch 59/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.5351\n",
      "Epoch 60/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.5235\n",
      "\n",
      "Epoch 00060: loss improved from 2.68489 to 2.52353, saving model to checkpoint-60-2.5235.h5\n",
      "Epoch 61/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.5133\n",
      "Epoch 62/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.4954\n",
      "Epoch 63/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.4783\n",
      "Epoch 64/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.4706\n",
      "Epoch 65/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.4590\n",
      "Epoch 66/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 2.4339\n",
      "Epoch 67/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.4265\n",
      "Epoch 68/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.3969\n",
      "Epoch 69/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.3882\n",
      "Epoch 70/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.3687\n",
      "Epoch 71/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.3474\n",
      "Epoch 72/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.3318\n",
      "Epoch 73/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.3160\n",
      "Epoch 74/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.2694\n",
      "Epoch 75/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.2420\n",
      "Epoch 76/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.2312\n",
      "Epoch 77/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.2089\n",
      "Epoch 78/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.1811\n",
      "Epoch 79/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.1461\n",
      "Epoch 80/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.1237\n",
      "\n",
      "Epoch 00080: loss improved from 2.52353 to 2.12372, saving model to checkpoint-80-2.1237.h5\n",
      "Epoch 81/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.0836\n",
      "Epoch 82/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.0670\n",
      "Epoch 83/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.0467\n",
      "Epoch 84/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 2.0301\n",
      "Epoch 85/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.9933\n",
      "Epoch 86/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.9489\n",
      "Epoch 87/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.9138\n",
      "Epoch 88/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.8988\n",
      "Epoch 89/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.8962\n",
      "Epoch 90/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.8441\n",
      "Epoch 91/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.8068\n",
      "Epoch 92/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 1.7681\n",
      "Epoch 93/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.7454\n",
      "Epoch 94/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.7190\n",
      "Epoch 95/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.6793\n",
      "Epoch 96/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 1.6452\n",
      "Epoch 97/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 1.6183\n",
      "Epoch 98/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 1.6004\n",
      "Epoch 99/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.5957\n",
      "Epoch 100/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.5549\n",
      "\n",
      "Epoch 00100: loss improved from 2.12372 to 1.55493, saving model to checkpoint-100-1.5549.h5\n",
      "Epoch 101/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.5097\n",
      "Epoch 102/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.4825\n",
      "Epoch 103/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.4669\n",
      "Epoch 104/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.4284\n",
      "Epoch 105/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.4209\n",
      "Epoch 106/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.3943\n",
      "Epoch 107/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.3579\n",
      "Epoch 108/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.3334\n",
      "Epoch 109/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.3542\n",
      "Epoch 110/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.2964\n",
      "Epoch 111/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.2602\n",
      "Epoch 112/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.2321\n",
      "Epoch 113/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.1992\n",
      "Epoch 114/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.1977\n",
      "Epoch 115/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.1585\n",
      "Epoch 116/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.1416\n",
      "Epoch 117/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.1484\n",
      "Epoch 118/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.1179\n",
      "Epoch 119/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.0864\n",
      "Epoch 120/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.0582\n",
      "\n",
      "Epoch 00120: loss improved from 1.55493 to 1.05821, saving model to checkpoint-120-1.0582.h5\n",
      "Epoch 121/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.0308\n",
      "Epoch 122/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.0248\n",
      "Epoch 123/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.0305\n",
      "Epoch 124/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 1.0039\n",
      "Epoch 125/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.9668\n",
      "Epoch 126/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.9631\n",
      "Epoch 127/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.9195\n",
      "Epoch 128/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.9157\n",
      "Epoch 129/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.9030\n",
      "Epoch 130/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.8901\n",
      "Epoch 131/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.8808\n",
      "Epoch 132/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.8560\n",
      "Epoch 133/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.8491\n",
      "Epoch 134/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.8251\n",
      "Epoch 135/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.8312\n",
      "Epoch 136/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.8129\n",
      "Epoch 137/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.7858\n",
      "Epoch 138/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.7664\n",
      "Epoch 139/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.7521\n",
      "Epoch 140/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.7468\n",
      "\n",
      "Epoch 00140: loss improved from 1.05821 to 0.74677, saving model to checkpoint-140-0.7468.h5\n",
      "Epoch 141/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.7614\n",
      "Epoch 142/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.7287\n",
      "Epoch 143/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.7221\n",
      "Epoch 144/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.7013\n",
      "Epoch 145/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.6758\n",
      "Epoch 146/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.6790\n",
      "Epoch 147/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.6785\n",
      "Epoch 148/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.6658\n",
      "Epoch 149/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.6523\n",
      "Epoch 150/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.6452\n",
      "Epoch 151/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.6166\n",
      "Epoch 152/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.6258\n",
      "Epoch 153/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.6053\n",
      "Epoch 154/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.6095\n",
      "Epoch 155/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.5883\n",
      "Epoch 156/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.5789\n",
      "Epoch 157/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.5836\n",
      "Epoch 158/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.5799\n",
      "Epoch 159/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.5821\n",
      "Epoch 160/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.5615\n",
      "\n",
      "Epoch 00160: loss improved from 0.74677 to 0.56148, saving model to checkpoint-160-0.5615.h5\n",
      "Epoch 161/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.5614\n",
      "Epoch 162/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.5636\n",
      "Epoch 163/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.5254\n",
      "Epoch 164/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.5176\n",
      "Epoch 165/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.5082\n",
      "Epoch 166/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.5143\n",
      "Epoch 167/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.5102\n",
      "Epoch 168/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4867\n",
      "Epoch 169/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4721\n",
      "Epoch 170/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4689\n",
      "Epoch 171/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4852\n",
      "Epoch 172/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4679\n",
      "Epoch 173/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4546\n",
      "Epoch 174/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4574\n",
      "Epoch 175/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4491\n",
      "Epoch 176/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4515\n",
      "Epoch 177/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4314\n",
      "Epoch 178/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4320\n",
      "Epoch 179/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4213\n",
      "Epoch 180/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4270\n",
      "\n",
      "Epoch 00180: loss improved from 0.56148 to 0.42702, saving model to checkpoint-180-0.4270.h5\n",
      "Epoch 181/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4396\n",
      "Epoch 182/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4078\n",
      "Epoch 183/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4039\n",
      "Epoch 184/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.4063\n",
      "Epoch 185/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3949\n",
      "Epoch 186/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3951\n",
      "Epoch 187/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3911\n",
      "Epoch 188/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3804\n",
      "Epoch 189/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3719\n",
      "Epoch 190/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3832\n",
      "Epoch 191/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3948\n",
      "Epoch 192/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3732\n",
      "Epoch 193/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3737\n",
      "Epoch 194/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3683\n",
      "Epoch 195/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3563\n",
      "Epoch 196/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3523\n",
      "Epoch 197/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3420\n",
      "Epoch 198/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3463\n",
      "Epoch 199/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3408\n",
      "Epoch 200/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3457\n",
      "\n",
      "Epoch 00200: loss improved from 0.42702 to 0.34567, saving model to checkpoint-200-0.3457.h5\n",
      "Epoch 201/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3548\n",
      "Epoch 202/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3394\n",
      "Epoch 203/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3391\n",
      "Epoch 204/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3131\n",
      "Epoch 205/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3192\n",
      "Epoch 206/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3318\n",
      "Epoch 207/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.3251\n",
      "Epoch 208/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3152\n",
      "Epoch 209/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3070\n",
      "Epoch 210/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.3110\n",
      "Epoch 211/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.3006\n",
      "Epoch 212/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.3168\n",
      "Epoch 213/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.3112\n",
      "Epoch 214/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3135\n",
      "Epoch 215/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3029\n",
      "Epoch 216/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3066\n",
      "Epoch 217/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2880\n",
      "Epoch 218/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.2946\n",
      "Epoch 219/500\n",
      "3193/3193 [==============================] - 9s 3ms/step - loss: 0.2981\n",
      "Epoch 220/500\n",
      "3193/3193 [==============================] - 8s 3ms/step - loss: 0.2798\n",
      "\n",
      "Epoch 00220: loss improved from 0.34567 to 0.27981, saving model to checkpoint-220-0.2798.h5\n",
      "Epoch 221/500\n",
      "3193/3193 [==============================] - 8s 2ms/step - loss: 0.3014\n",
      "Epoch 222/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.2698\n",
      "Epoch 223/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.2725\n",
      "Epoch 224/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.3002\n",
      "Epoch 225/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3004\n",
      "Epoch 226/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2798\n",
      "Epoch 227/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2903\n",
      "Epoch 228/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.3036\n",
      "Epoch 229/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2694\n",
      "Epoch 230/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2544\n",
      "Epoch 231/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2624\n",
      "Epoch 232/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2583\n",
      "Epoch 233/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2633\n",
      "Epoch 234/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2663\n",
      "Epoch 235/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2448\n",
      "Epoch 236/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2499\n",
      "Epoch 237/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2482\n",
      "Epoch 238/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2614\n",
      "Epoch 239/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2643\n",
      "Epoch 240/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2522\n",
      "\n",
      "Epoch 00240: loss improved from 0.27981 to 0.25217, saving model to checkpoint-240-0.2522.h5\n",
      "Epoch 241/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2404\n",
      "Epoch 242/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.2377\n",
      "Epoch 243/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2337\n",
      "Epoch 244/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2349\n",
      "Epoch 245/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2402\n",
      "Epoch 246/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2301\n",
      "Epoch 247/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2293\n",
      "Epoch 248/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2179\n",
      "Epoch 249/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.2190\n",
      "Epoch 250/500\n",
      "3193/3193 [==============================] - 8s 3ms/step - loss: 0.2217\n",
      "Epoch 251/500\n",
      "3193/3193 [==============================] - 8s 3ms/step - loss: 0.2171\n",
      "Epoch 252/500\n",
      "3193/3193 [==============================] - 8s 3ms/step - loss: 0.2405\n",
      "Epoch 253/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.2175\n",
      "Epoch 254/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.2232\n",
      "Epoch 255/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.2171\n",
      "Epoch 256/500\n",
      "3193/3193 [==============================] - 9s 3ms/step - loss: 0.2210\n",
      "Epoch 257/500\n",
      "3193/3193 [==============================] - 8s 3ms/step - loss: 0.2201\n",
      "Epoch 258/500\n",
      "3193/3193 [==============================] - 9s 3ms/step - loss: 0.2420\n",
      "Epoch 259/500\n",
      "3193/3193 [==============================] - 11s 3ms/step - loss: 0.2201\n",
      "Epoch 260/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.2316\n",
      "\n",
      "Epoch 00260: loss improved from 0.25217 to 0.23157, saving model to checkpoint-260-0.2316.h5\n",
      "Epoch 261/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.2120\n",
      "Epoch 262/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.2137\n",
      "Epoch 263/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2001\n",
      "Epoch 264/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.2033\n",
      "Epoch 265/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2275\n",
      "Epoch 266/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2342\n",
      "Epoch 267/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2012\n",
      "Epoch 268/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.2126\n",
      "Epoch 269/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2172\n",
      "Epoch 270/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2043\n",
      "Epoch 271/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.2041\n",
      "Epoch 272/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2088\n",
      "Epoch 273/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2134\n",
      "Epoch 274/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2066\n",
      "Epoch 275/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1941\n",
      "Epoch 276/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1958\n",
      "Epoch 277/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2067\n",
      "Epoch 278/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2240\n",
      "Epoch 279/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2180\n",
      "Epoch 280/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1831\n",
      "\n",
      "Epoch 00280: loss improved from 0.23157 to 0.18314, saving model to checkpoint-280-0.1831.h5\n",
      "Epoch 281/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1942\n",
      "Epoch 282/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1957\n",
      "Epoch 283/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1839\n",
      "Epoch 284/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1956\n",
      "Epoch 285/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1873\n",
      "Epoch 286/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1952\n",
      "Epoch 287/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2005\n",
      "Epoch 288/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2012\n",
      "Epoch 289/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2115\n",
      "Epoch 290/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2124\n",
      "Epoch 291/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.2038\n",
      "Epoch 292/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1966\n",
      "Epoch 293/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1742\n",
      "Epoch 294/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1830\n",
      "Epoch 295/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1796\n",
      "Epoch 296/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1695\n",
      "Epoch 297/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1761\n",
      "Epoch 298/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1679\n",
      "Epoch 299/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1679\n",
      "Epoch 300/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1760\n",
      "\n",
      "Epoch 00300: loss improved from 0.18314 to 0.17599, saving model to checkpoint-300-0.1760.h5\n",
      "Epoch 301/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1719\n",
      "Epoch 302/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1849\n",
      "Epoch 303/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1665\n",
      "Epoch 304/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1578\n",
      "Epoch 305/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1655\n",
      "Epoch 306/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.2019\n",
      "Epoch 307/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1703\n",
      "Epoch 308/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1721\n",
      "Epoch 309/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1539\n",
      "Epoch 310/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1643\n",
      "Epoch 311/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1820\n",
      "Epoch 312/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1749\n",
      "Epoch 313/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1727\n",
      "Epoch 314/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1639\n",
      "Epoch 315/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.1908\n",
      "Epoch 316/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1720\n",
      "Epoch 317/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1749\n",
      "Epoch 318/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1658\n",
      "Epoch 319/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1562\n",
      "Epoch 320/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1679\n",
      "\n",
      "Epoch 00320: loss improved from 0.17599 to 0.16795, saving model to checkpoint-320-0.1679.h5\n",
      "Epoch 321/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1538\n",
      "Epoch 322/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1690\n",
      "Epoch 323/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1739\n",
      "Epoch 324/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1776\n",
      "Epoch 325/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1504\n",
      "Epoch 326/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1604\n",
      "Epoch 327/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1566\n",
      "Epoch 328/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1551\n",
      "Epoch 329/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1639\n",
      "Epoch 330/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1604\n",
      "Epoch 331/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1588\n",
      "Epoch 332/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1501\n",
      "Epoch 333/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1539\n",
      "Epoch 334/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1454\n",
      "Epoch 335/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1522\n",
      "Epoch 336/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1619\n",
      "Epoch 337/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1632\n",
      "Epoch 338/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1536\n",
      "Epoch 339/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1426\n",
      "Epoch 340/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1559\n",
      "\n",
      "Epoch 00340: loss improved from 0.16795 to 0.15586, saving model to checkpoint-340-0.1559.h5\n",
      "Epoch 341/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1527\n",
      "Epoch 342/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1466\n",
      "Epoch 343/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1432\n",
      "Epoch 344/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1887\n",
      "Epoch 345/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1761\n",
      "Epoch 346/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1413\n",
      "Epoch 347/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1652\n",
      "Epoch 348/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1594\n",
      "Epoch 349/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1388\n",
      "Epoch 350/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1522\n",
      "Epoch 351/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1470\n",
      "Epoch 352/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1571\n",
      "Epoch 353/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1444\n",
      "Epoch 354/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1432\n",
      "Epoch 355/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1501\n",
      "Epoch 356/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1628\n",
      "Epoch 357/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1525\n",
      "Epoch 358/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1372\n",
      "Epoch 359/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1498\n",
      "Epoch 360/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1485\n",
      "\n",
      "Epoch 00360: loss improved from 0.15586 to 0.14851, saving model to checkpoint-360-0.1485.h5\n",
      "Epoch 361/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1561\n",
      "Epoch 362/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1455\n",
      "Epoch 363/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1429\n",
      "Epoch 365/500\n",
      "3193/3193 [==============================] - 9s 3ms/step - loss: 0.1505\n",
      "Epoch 366/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.1364\n",
      "Epoch 367/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.1543\n",
      "Epoch 368/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1416\n",
      "Epoch 369/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1569\n",
      "Epoch 370/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1678A: 0s - loss: 0.170\n",
      "Epoch 371/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1509\n",
      "Epoch 372/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1503\n",
      "Epoch 373/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1389\n",
      "Epoch 374/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1274\n",
      "Epoch 375/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1286\n",
      "Epoch 376/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1263\n",
      "Epoch 377/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1612\n",
      "Epoch 378/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1290\n",
      "Epoch 379/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1356\n",
      "Epoch 380/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1179\n",
      "\n",
      "Epoch 00380: loss improved from 0.14851 to 0.11792, saving model to checkpoint-380-0.1179.h5\n",
      "Epoch 381/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1234\n",
      "Epoch 382/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1384\n",
      "Epoch 383/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1344\n",
      "Epoch 384/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1478\n",
      "Epoch 385/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1567\n",
      "Epoch 386/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1420\n",
      "Epoch 387/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1277\n",
      "Epoch 388/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1300\n",
      "Epoch 389/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1308\n",
      "Epoch 390/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1338\n",
      "Epoch 391/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1382\n",
      "Epoch 392/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1415\n",
      "Epoch 393/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1205\n",
      "Epoch 394/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1360\n",
      "Epoch 395/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1254\n",
      "Epoch 396/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1327\n",
      "Epoch 397/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1251\n",
      "Epoch 398/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1251\n",
      "Epoch 399/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1192\n",
      "Epoch 400/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1308\n",
      "\n",
      "Epoch 00400: loss did not improve from 0.11792\n",
      "Epoch 401/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1509\n",
      "Epoch 402/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1525\n",
      "Epoch 403/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1291\n",
      "Epoch 404/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1336\n",
      "Epoch 405/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1286\n",
      "Epoch 406/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1322\n",
      "Epoch 407/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1302\n",
      "Epoch 408/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1275\n",
      "Epoch 409/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1286\n",
      "Epoch 410/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1182\n",
      "Epoch 411/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1251\n",
      "Epoch 412/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1357\n",
      "Epoch 413/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1296\n",
      "Epoch 414/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1350\n",
      "Epoch 415/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1183\n",
      "Epoch 416/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1384\n",
      "Epoch 417/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1335\n",
      "Epoch 418/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1161\n",
      "Epoch 419/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1267\n",
      "Epoch 420/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.1089\n",
      "\n",
      "Epoch 00420: loss improved from 0.11792 to 0.10890, saving model to checkpoint-420-0.1089.h5\n",
      "Epoch 421/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.1246\n",
      "Epoch 422/500\n",
      "3193/3193 [==============================] - 7s 2ms/step - loss: 0.1427\n",
      "Epoch 423/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1210\n",
      "Epoch 424/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1289\n",
      "Epoch 425/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1218\n",
      "Epoch 426/500\n",
      "3193/3193 [==============================] - 6s 2ms/step - loss: 0.1254\n",
      "Epoch 427/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1342\n",
      "Epoch 428/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1312\n",
      "Epoch 429/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1222\n",
      "Epoch 430/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1175\n",
      "Epoch 431/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1215\n",
      "Epoch 432/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1105\n",
      "Epoch 433/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1387\n",
      "Epoch 434/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1221\n",
      "Epoch 435/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1209\n",
      "Epoch 436/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1125\n",
      "Epoch 437/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1278\n",
      "Epoch 438/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1163\n",
      "Epoch 439/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1075\n",
      "Epoch 440/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1144\n",
      "\n",
      "Epoch 00440: loss did not improve from 0.10890\n",
      "Epoch 441/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1126\n",
      "Epoch 442/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1384\n",
      "Epoch 443/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1379\n",
      "Epoch 444/500\n",
      "3193/3193 [==============================] - 5s 2ms/step - loss: 0.1198\n",
      "Epoch 445/500\n",
      "2304/3193 [====================>.........] - ETA: 1s - loss: 0.1216"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a1c12cd4beaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
